# agent_006 Benchmark Results

**Symbol:** COINBASE_SPOT_BTC_USD
**Start Time:** 2026-01-06T19:53:10.959Z
**Progress:** Round 2/12 (Phase 0)
**Last Updated:** 2026-01-06T20:00:04.759Z

## Run Configuration

| Setting | Value |
|---------|-------|
| Tolerance | 0% strict undercut |
| Unique snapTimes | 12 |
| Models tested | 28 |

**Per-Horizon Configuration:**

| Horizon | Bar Size | Lookback Bars | Horizon Bars |
|---------|----------|---------------|--------------|
| 15m | 5m | 24 | 3 |
| 1h | 15m | 32 | 4 |
| 4h | 1h | 32 | 4 |
| 24h | 4h | 48 | 6 |

## Benchmark Overview

This benchmark evaluates LLMs on a **binary classification task** across 4 horizons (15m, 1h, 4h, 24h):

> For each horizon, predict whether the current reference low will hold (*no new low*) or be undercut within the forward window.

**Label definition (`noNewLow`):**
- `1` (true): Forward window low ‚â• reference low (bottom held)
- `0` (false): Forward window low < reference low (new low made)

**Horizons** share the same symbol and time but differ in bar size, lookback window, and forward prediction window.

## Methodology

### Ground Truth
- **Reference low**: Minimum low price across lookback candles
- **Forward low**: Minimum low price in the forward window (prediction horizon)
- **Label**: `y = 1` if forward low ‚â• reference low, else `y = 0`

### Probability Mapping
Models output `{ noNewLow: boolean; confidence ‚àà [0.5, 1.0] }` per horizon.
- Probability of no new low: `p = noNewLow ? confidence : (1 - confidence)`

### Scoring
- **Log loss** (primary): `LL = -(y¬∑log(p) + (1‚àíy)¬∑log(1‚àíp))`, with p clipped to [Œµ, 1‚àíŒµ]
- **Random baseline**: p=0.5 gives LL ‚âà 0.693
- **Brier score**: Used in Phase 0 sanity checks only (not shown in tables)

### Phases & Elimination
- **Phase 0 ‚Äì Sanity filter**: Disqualifies horizons where model log loss > random baseline √ó 1.1 (‚âà0.762), shows degenerate predictions (all mapped p ‚â• 0.9 or p ‚â§ 0.1), or has high extreme error rate (>20% confident wrong predictions where p > 0.8 but actual = false)
- **Phase 1 ‚Äì Percentile filter**: Retains models above performance threshold per horizon
- **Phase 2 ‚Äì Stability filter**: Evaluates consistency using rolling windows; eliminates models with no qualified horizons remaining
- **Phase 3 ‚Äì Final ranking**: Composite scoring of surviving models

> **Quick mode note:** Verification runs apply the same Phase 0‚Äì3 scoring pipeline as full benchmarks. With limited samples, rankings are indicative only and should not be used for final model selection.

**Status codes:**
- `‚úÖ Active`: Survived all phases with ‚â•1 qualified horizon
- `‚ùå P0`: Eliminated in Phase 0 (all horizons failed sanity checks)
- `‚ùå P2`: Eliminated in Phase 2 (no qualified horizons remaining)

## Summary

- **Active Models:** 28
- **Eliminated:** 0
- **Models with Failures:** 8

## Final Standings (Survivors)

*No models survived all elimination phases with adequate coverage.*

## All Models (Research Reference)

*Rankings are by composite score among models with adequate coverage (‚â•80% and ‚â•10 rounds).*

*No models have adequate coverage (‚â•80% and ‚â•10 rounds).*

### Not Ranked (Low Coverage or Early Stopped)

*These models had <80% coverage OR <10 effective rounds and are shown for reference only, not as competitive rankings.*

| Model | Status | Rnds | Cov | 15m | 1h | 4h | 24h | Mean | %Rank | BestWin | Stabil | TtP | Score |
|-------|--------|------|-----|-----|-----|-----|-----|------|-------|---------|--------|-----|-------|
| google/gemini-2.5-pro | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.134 | üî¥0.837 | üü¢0.078 | üü¢0.036 | üü¢0.271 | 100.0 | 0.059 | 0.429 | 0.50 | 0.8554 |
| google/gemini-2.5-flash | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.193 | üî¥0.837 | üü¢0.078 | üü¢0.036 | üü¢0.286 | 95.0 | 0.041 | 0.425 | 0.50 | 0.8389 |
| google/gemini-3-pro-preview | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.193 | üî¥0.871 | üü¢0.105 | üü¢0.078 | üü¢0.312 | 90.0 | 0.087 | 0.416 | 0.50 | 0.8138 |
| xai/grok-4 | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.197 | üü°0.714 | üü¢0.260 | üü¢0.308 | üü¢0.369 | 85.0 | 0.205 | 0.340 | 0.50 | 0.7911 |
| openai/gpt-5.2 | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.268 | üü°0.635 | üü¢0.406 | üü¢0.362 | üü¢0.418 | 80.0 | 0.262 | 0.254 | 0.50 | 0.7800 |
| mistral/pixtral-large-latest | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.134 | üî¥1.060 | üü¢0.255 | üü¢0.322 | üü¢0.443 | 75.0 | 0.164 | 0.554 | 0.50 | 0.7146 |
| openai/gpt-4.1 | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.315 | üî¥0.983 | üü¢0.268 | üü¢0.225 | üü¢0.448 | 70.0 | 0.233 | 0.446 | 0.50 | 0.7058 |
| anthropic/claude-3-5-haiku-latest | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.357 | üü°0.636 | üü¢0.308 | üü°0.514 | üü¢0.454 | 60.0 | 0.312 | 0.268 | 0.50 | 0.6896 |
| xai/grok-4.1-fast-reasoning | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üî¥0.817 | üü¢0.322 | üü¢0.193 | üü°0.511 | üü¢0.461 | 55.0 | 0.224 | 0.366 | 0.50 | 0.6631 |
| mistral/ministral-8b-latest | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.134 | üî¥1.177 | üü¢0.193 | üü¢0.288 | üü¢0.448 | 65.0 | 0.106 | 0.705 | 0.50 | 0.6530 |
| xai/grok-2-vision | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.255 | üü°0.669 | üü°0.636 | üü°0.562 | üü°0.531 | 45.0 | 0.266 | 0.295 | 0.50 | 0.6310 |
| perplexity/sonar-pro | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.337 | üü°0.714 | üü¢0.268 | üü°0.746 | üü°0.516 | 50.0 | 0.214 | 0.472 | 0.50 | 0.6234 |
| anthropic/claude-opus-4-5 | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.193 | üî¥0.837 | üü°0.674 | üü¢0.471 | üü°0.544 | 40.0 | 0.224 | 0.386 | 0.50 | 0.5992 |
| anthropic/claude-3-5-sonnet-20241022 | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.288 | üü°0.714 | üü°0.703 | üü°0.674 | üü°0.595 | 30.0 | 0.266 | 0.369 | 0.50 | 0.5562 |
| google/gemini-2.5-flash-lite | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.357 | üî¥0.805 | üü°0.511 | üü°0.628 | üü°0.575 | 35.0 | 0.312 | 0.485 | 0.50 | 0.5461 |
| anthropic/claude-sonnet-4-5 | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.329 | üü°0.763 | üî¥1.009 | üü¢0.288 | üü°0.597 | 25.0 | 0.348 | 0.357 | 0.50 | 0.5266 |
| google/gemini-2.0-flash | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.357 | üü°0.714 | üî¥0.857 | üî¥0.983 | üü°0.728 | 20.0 | 0.408 | 0.440 | 0.50 | 0.4808 |
| xai/grok-4-fast-non-reasoning | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.290 | üî¥0.909 | üü°0.674 | üî¥1.151 | üü°0.756 | 15.0 | 0.337 | 0.433 | 0.50 | 0.4728 |
| anthropic/claude-3-7-sonnet-latest | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.322 | üü°0.718 | üü°0.780 | üî¥1.204 | üü°0.756 | 10.0 | 0.343 | 0.430 | 0.50 | 0.4525 |
| anthropic/claude-haiku-4-5 | ‚úÖ Active | 2 | 8/48 (17%)‚ö†Ô∏è | üü¢0.329 | üü°0.763 | üü°0.746 | üî¥1.309 | üü°0.787 | 5.0 | 0.348 | 0.458 | 0.50 | 0.4264 |

**Legend:**

*Log loss color coding:*
- üü¢ Good (‚â§ 0.5) | üü° OK (‚â§ 0.8) | üî¥ Poor (> 0.8)

*Column definitions:*
- `Rnds`: Number of successful rounds (failed rounds are excluded from metrics)
- `Cov`: Coverage as effective/intended (percent). ‚ö†Ô∏è indicates <80% coverage or <10 effective rounds on any horizon
- `15m, 1h, 4h, 24h`: Mean log loss for that horizon across all valid rounds
- `Mean`: Arithmetic mean of the four horizon log losses
- `%Rank`: Percentile rank among all models by composite Score (higher = better)
- `BestWin`: Best rolling-window average log loss (lower = better)
- `Stabil`: Standard deviation of per-round log loss (lower = better)
- `TtP`: Time-to-pivot ratio (lower = better). *Note: With the current no-new-low ground truth system, timing data is not available; all models show TtP = 0.50.*
- `Score`: Composite metric combining rank, best window, stability, and timing (40% rank + 30% bestWin‚Åª¬π + 20% stabil‚Åª¬π + 10% TtP‚Åª¬π). *Non-rankable horizons (insufficient label diversity) are excluded from composite score calculation.*

## Per-Horizon Rankings (Top 10)

*No ranking data available.*

## Model Failures

*Note: Failed rounds (API errors, malformed responses) are excluded from scoring. The `Rnds` column shows successful rounds used in metric calculation.*

| Model | Failed Rounds |
|-------|---------------|
| openai/gpt-4o | 1, 2 |
| openai/gpt-4o-mini | 1, 2 |
| openai/gpt-4.1-mini | 1, 2 |
| openai/gpt-5 | 1, 2 |
| openai/gpt-5-mini | 1, 2 |
| openai/gpt-5-nano | 1, 2 |
| mistral/pixtral-12b-2409 | 1, 2 |
| mistral/ministral-3b-latest | 1, 2 |

---
*Auto-generated by agent_006 benchmark*